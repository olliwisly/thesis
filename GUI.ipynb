{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical User Interface (GUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pls read the \"ReadMe.txt\" file first)\n",
    "\n",
    "To be able to demonstrate the improvement on drug prescription processing I developed a GUI.\n",
    "\n",
    "The architecture of the corresponding processing I have implemented in my thesis is explained in my thesis chapter 3 at the beginning.\n",
    "\n",
    "Analysis of the GUI are discussed and presented in chapter 5.\n",
    "\n",
    "All the implementations can be found within the following jupyter code using .txt files of the folder. All these have been implemented in the other jupyter files using tools explained in the thesis.\n",
    "\n",
    "_____________________________________________________________\n",
    "\n",
    "Within the code you can change from general (no habit) data to habit data, see in code.\n",
    "\n",
    "For general use, if you want to use interactions you have to tick the check box first or \"clear all\" before swapping between no interactions and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "import difflib\n",
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "import random\n",
    "from random import randrange\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "csv.field_size_limit(1000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code two options are available.\n",
    "\n",
    "- The standard option would be not including habit data, and therefore using randomly ordered data in the process of synthetisation. (set \"habit=0\")\n",
    "\n",
    "- In Chapter 6 of the thesis I'm assuming that humans incorporate their habits to the way of prescribing drugs, following that I used and different class order within my data synthetisation which will be used if setting habit to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "habit=0\n",
    "\n",
    "#1.. use habit data (order predefined)\n",
    "#0.. use normal data (order ranomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if habit==0:\n",
    "    with open(\"allpre.txt\", \"rb\") as fp:   # Unpickling\n",
    "        allprescriptions = pickle.load(fp)\n",
    "\n",
    "    with open(\"allrealpre.txt\",\"rb\") as fp:\n",
    "        allrealprescriptions = pickle.load(fp)\n",
    "\n",
    "    with open(\"fold_data.txt\",\"rb\") as fp:\n",
    "        fold_data=pickle.load(fp)\n",
    "\n",
    "else:\n",
    "    with open(\"allpre_h.txt\", \"rb\") as fp:   # Unpickling\n",
    "        allprescriptions = pickle.load(fp)\n",
    "\n",
    "    with open(\"allrealpre_h.txt\",\"rb\") as fp:\n",
    "        allrealprescriptions = pickle.load(fp)\n",
    "\n",
    "    with open(\"fold_data_h.txt\",\"rb\") as fp:\n",
    "        fold_data=pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'syndata' in globals():\n",
    "    del syndata\n",
    "syndata=[]\n",
    "for i in range(len(fold_data)):\n",
    "    syndata.extend(fold_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the general data extraction for the general doses was done in the other jupyter notebooks, the interactions have been extracted and analysed in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=[]\n",
    "with open('interactions.csv', 'r',newline='',encoding=\"utf-8\") as csvfile:  \n",
    "    csvreader = csv.reader(csvfile,quoting=csv.QUOTE_ALL)\n",
    "    for row in csvreader:\n",
    "        alldata.append(row)\n",
    "if len(alldata)==1:\n",
    "    alldata=alldata[0]\n",
    "\n",
    "interactions={}\n",
    "intlist=[]\n",
    "for i in range(0,len(alldata),2):\n",
    "    #string dictionary to dictionary:\n",
    "    interactions[alldata[i]]=ast.literal_eval(alldata[i+1])\n",
    "\n",
    "    #save all interaction drug names in extra list\n",
    "    for k,v in interactions[alldata[i]].items():\n",
    "        intlist.append(k)\n",
    "\n",
    "random.seed(5)\n",
    "#randomly select if a patient already takes 1-10 drugs\n",
    "n_drugs=randrange(10)\n",
    "curr_patient=[]\n",
    "severe_interactions=[]\n",
    "moderate_interactions=[]\n",
    "while len(curr_patient)<n_drugs:\n",
    "    drug=randrange(len(intlist))\n",
    "    if intlist[drug] not in curr_patient:\n",
    "        curr_patient.append(intlist[drug])\n",
    "        for keys in interactions.keys():\n",
    "            if intlist[drug] in interactions[keys].keys():\n",
    "                severity=interactions[keys][intlist[drug]][0]\n",
    "                if severity=='Severe':\n",
    "                    if keys not in severe_interactions:\n",
    "                        severe_interactions.append(keys)\n",
    "                elif severity=='Moderate':\n",
    "                    if keys not in moderate_interactions:\n",
    "                        moderate_interactions.append(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonyms are used for possible input corrections in the suggestion list, using all available generated prescriptions and the noise of it. Implementation of the dictionary from noised to denoised data can be found in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms={}\n",
    "for i in range(len(allrealprescriptions)):\n",
    "    for j in range(len(allrealprescriptions[i])):\n",
    "        if allrealprescriptions[i][j][0] not in synonyms.keys():\n",
    "            try:\n",
    "                np.int(allrealprescriptions[i][j][0][0])\n",
    "            except:\n",
    "                synonyms[allrealprescriptions[i][j][0]]=allrealprescriptions[i][j][0]\n",
    "        if allprescriptions[i][j][0]!=allrealprescriptions[i][j][0]:\n",
    "            if allprescriptions[i][j][0] not in synonyms.keys():\n",
    "                try:\n",
    "                    np.int(allprescriptions[i][j][0][0])\n",
    "                except:\n",
    "                    synonyms[allprescriptions[i][j][0]]=allrealprescriptions[i][j][0]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagorder=['DN',['DO','UN'],'ST','FR','RO','MA','MI','DU']\n",
    "tagorder_all=[]\n",
    "for j in range(len(syndata)):\n",
    "    tagorder_all.append([])\n",
    "    curr_tag=[tagorder[x] for x in syndata[j][18]]\n",
    "    for i in curr_tag:\n",
    "        tag_ind=tagorder.index(i)\n",
    "        if tag_ind>1:\n",
    "            tag_ind+=1\n",
    "        if syndata[j][tag_ind]!='':\n",
    "            class_n=len(syndata[j][tag_ind].split())\n",
    "            if class_n>1:\n",
    "                tagorder_all[-1].extend([i for k in range(class_n)])\n",
    "                continue\n",
    "            if type(i)==list:\n",
    "                tagorder_all[-1].extend(i)\n",
    "                continue\n",
    "            tagorder_all[-1].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word_class={'DN':0,'DO':0,'UN':0,'ST':0,'MA':0,'MI':0,'DU':0,'FR':0,'RO':0}\n",
    "for i in range(len(tagorder_all)):\n",
    "    first_word_class[tagorder_all[i][0]]+=1\n",
    "\n",
    "all_grams={}\n",
    "\n",
    "for i in range(len(tagorder_all)):\n",
    "    curr_tag_classes=tagorder_all[i]\n",
    "    curr_grams=[]\n",
    "    curr_grams.extend(ngrams(curr_tag_classes,2)) #bigrams\n",
    "    curr_grams.extend(ngrams(curr_tag_classes,3)) #trigrams\n",
    "    curr_grams.extend(ngrams(curr_tag_classes,4)) #4-grams\n",
    "    curr_grams.extend(ngrams(curr_tag_classes,5)) #5-grams\n",
    "    for i in curr_grams:\n",
    "        class_concat=\" \".join(i)\n",
    "        if class_concat in all_grams.keys():\n",
    "            all_grams[class_concat]+=1\n",
    "        else:\n",
    "            all_grams[class_concat]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_grams=[]\n",
    "for i in range(len(tagorder_all)):\n",
    "    for j in range(2,len(tagorder_all[i])):\n",
    "        tag_grams.append(tagorder_all[i][:j])\n",
    "    tag_grams.append(tagorder_all[i])\n",
    "\n",
    "gram_dict={}\n",
    "for i in range(len(tag_grams)):\n",
    "    class_join=' '.join(tag_grams[i])\n",
    "    if class_join in gram_dict.keys():\n",
    "        gram_dict[class_join]+=1\n",
    "    else:\n",
    "        gram_dict[class_join]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagg=PerceptronTagger(load=False)\n",
    "tagg.train(allprescriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_class(inpstr):\n",
    "    if inpstr=='':\n",
    "        currtag=[('','')]\n",
    "    currtag=tagg.tag(inpstr.split())\n",
    "    \n",
    "    currstr=' '.join([v for k,v in currtag])\n",
    "    \n",
    "    tagorder=['DN','DO','UN','ST','FR','RO','MA','MI','DU']\n",
    "    \n",
    "    prob={'DN':0,'DO':0,'UN':0,'ST':0,'FR':0,'RO':0,'MA':0,'MI':0,'DU':0}\n",
    "    \n",
    "    if currstr=='':\n",
    "        for i in tagorder:\n",
    "            prob[i]=first_word_class[i]\n",
    "        sumprobs=sum(prob.values())\n",
    "        if sumprobs==0:\n",
    "            return 0\n",
    "        else:\n",
    "            for i in tagorder:\n",
    "                prob[i]/=sumprobs\n",
    "        return prob\n",
    "    else:\n",
    "        possibletags=[]\n",
    "        for i in tagorder:\n",
    "            if i not in currstr:\n",
    "                possibletags.append(i)\n",
    "        possibletags.append(currstr.split()[-1])\n",
    "    \n",
    "    \n",
    "    currstr_split=currstr.split()\n",
    "    for i in range(1,min(5,len(currstr_split)+1)):\n",
    "        currprob={'DN':0,'DO':0,'UN':0,'ST':0,'FR':0,'RO':0,'MA':0,'MI':0,'DU':0}\n",
    "        \n",
    "        for j in possibletags:\n",
    "            try:\n",
    "                currprob[j]+=all_grams[\" \".join(currstr_split[-i:])+' '+j]\n",
    "            except:\n",
    "                currprob[j]+=0\n",
    "        sumprobs=sum(currprob.values())\n",
    "        if sumprobs==0:\n",
    "            continue\n",
    "        for j in possibletags:\n",
    "            prob[j]+=currprob[j]/sumprobs\n",
    "        \n",
    "    for j in possibletags: \n",
    "        prob[j]/=i\n",
    "    \n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize == only real words without classes!\n",
    "\n",
    "#all real words\n",
    "if 'tokenizedallpre' in globals():\n",
    "    del tokenizedallpre\n",
    "tokenizedallpre=[]\n",
    "tokenizedallwords_real=[]\n",
    "\n",
    "for i in range(len(allrealprescriptions)):\n",
    "    tokenizedallpre.append([])\n",
    "    for j in range(len(allrealprescriptions[i])):\n",
    "        tokenizedallpre[i].append(allrealprescriptions[i][j][0])\n",
    "        #for word autocomplete:\n",
    "        tokenizedallwords_real.append(allrealprescriptions[i][j][0])\n",
    "        \n",
    "        \n",
    "#all noised word characters but then used the realprescription words\n",
    "if 'tokenizedallwords' in globals():\n",
    "    del tokenizedallwords\n",
    "tokenizedallwords=[]\n",
    "\n",
    "for i in range(len(allprescriptions)):\n",
    "    for j in range(len(allprescriptions[i])):\n",
    "        tokenizedallwords.append(list(allprescriptions[i][j][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenizedallpre)\n",
    "\n",
    "model = MLE(n) # Lets train a 5-grams maximum likelihood estimation model.\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_chars={}\n",
    "for i in range(len(tokenizedallwords)):\n",
    "    currword=tokenizedallwords_real[i]\n",
    "    for j in range(1,len(tokenizedallwords[i])+1):\n",
    "        currword_part=''.join(tokenizedallwords[i][:j])\n",
    "        if currword_part not in train_data_chars.keys():\n",
    "            train_data_chars[currword_part]={}\n",
    "            train_data_chars[currword_part][currword]=1\n",
    "        else:\n",
    "            if currword not in train_data_chars[currword_part].keys():\n",
    "                train_data_chars[currword_part][currword]=1\n",
    "            else:\n",
    "                train_data_chars[currword_part][currword]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_complete(inpword):\n",
    "    try:\n",
    "        distr=train_data_chars[inpword]\n",
    "    except:\n",
    "        return(inpword)\n",
    "    if len(distr.keys())==1:\n",
    "        return(list(distr.keys())[0])\n",
    "    else:\n",
    "        overall=np.sum(list(distr.values()))\n",
    "        for k,v in distr.items():\n",
    "            distr[k]=v/overall\n",
    "        rand_nr=random.random()\n",
    "        currd=0\n",
    "        for val,k in enumerate(list(distr.keys())):\n",
    "            if rand_nr<=currd+distr[k]:\n",
    "                return(k)\n",
    "            else:\n",
    "                currd+=distr[k]\n",
    "    return(distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isvalidprescription(prescrip):\n",
    "    tagorder=['DN','DO','UN','ST','FR','RO','MA','MI','DU']\n",
    "    #0.. valid\n",
    "    #1.. invalid\n",
    "    \n",
    "    pos={}\n",
    "    for j in tagorder:\n",
    "        pos[j]=[]\n",
    "\n",
    "    bsp=prescrip\n",
    "    for i in range(len(bsp)):\n",
    "        pos[bsp[i][1]].append(i)\n",
    "\n",
    "    noprescription=0\n",
    "    for i in tagorder:\n",
    "        if len(pos[i])>1 and len(pos[i])!=len(range(pos[i][0],pos[i][-1]+1)):\n",
    "            noprescription=1\n",
    "    return(noprescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prescription_prediction(inpstr,newwords):\n",
    "    words=inpstr.strip().split()\n",
    "    count=model.counts[words[-1]]\n",
    "    if count==0:\n",
    "        return []\n",
    "    if inpstr[-1]!=' ':\n",
    "        inpstr=inpstr+' '\n",
    "    \n",
    "    final_output=[]\n",
    "    for i in range(newwords):\n",
    "        currgen_comb=inpstr\n",
    "        m=0\n",
    "        while m<15: #max wordnumber of the extension\n",
    "            n=0\n",
    "            output=[]\n",
    "            class_prob=next_word_class(currgen_comb)\n",
    "            if class_prob==0:\n",
    "                break\n",
    "            score=[]\n",
    "            while n<10:\n",
    "                currgen=model.generate(num_words=1,text_seed=currgen_comb.split())\n",
    "                if currgen[0]=='<':\n",
    "                    n+=1\n",
    "                    continue\n",
    "                if currgen in output:\n",
    "                    n+=1\n",
    "                    continue\n",
    "                currtag=tagg.tag(currgen_comb.split()+[currgen])\n",
    "                #check if probability!=0, last letter!=number,( at least one DN tag)\n",
    "                if class_prob[currtag[-1][1]]==0 or currtag[-1][0][-1].isdigit()==True:# or sum([ta=='DN' for nm,ta in currtag])==0:\n",
    "                    n+=1\n",
    "                    continue\n",
    "                \n",
    "                output.append(currgen)\n",
    "                n+=1\n",
    "            if len(output)==0:\n",
    "                break\n",
    "            elif len(output)==1:\n",
    "                currgen_comb=currgen_comb+output[0]+' '\n",
    "            else:\n",
    "                rand_index=random.randrange(len(output))\n",
    "                currgen_comb=currgen_comb+output[rand_index]+' '\n",
    "            m+=1\n",
    "        if currgen_comb.strip() not in final_output and inpstr.strip()!=currgen_comb.strip():\n",
    "            final_output.append(currgen_comb.strip())\n",
    "    return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(word1,word2):\n",
    "    difference=difflib.ndiff(word1.lower(),word2.lower())\n",
    "    plus=0\n",
    "    minus=0\n",
    "    for i in difference:\n",
    "        if i[0]=='-':\n",
    "            minus+=1\n",
    "        elif i[0]=='+':\n",
    "            plus+=1\n",
    "    return [minus,plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_prediction(inpstr,tagg,currentsuggestions,newwords):\n",
    "    word=inpstr.split()\n",
    "    \n",
    "    changeword=0\n",
    "    if inpstr[-1]==\" \":\n",
    "        changeword=1\n",
    "        output=[inpstr]\n",
    "    if changeword==0:\n",
    "        if len(word)<=1:\n",
    "            next_class=next_word_class('')\n",
    "        else:\n",
    "            next_class=next_word_class(' '.join(word[:-1])+' ')\n",
    "        output=[]\n",
    "        for i in range(10):\n",
    "            currstr=word_complete(word[-1])\n",
    "            if currstr==word[-1]:\n",
    "                break\n",
    "            output.append(currstr)\n",
    "            if len(word)<=1:\n",
    "                currtag=tagg.tag([output[-1]])\n",
    "            else:\n",
    "                currtag=tagg.tag(word[:-1]+[output[-1]])\n",
    "            try:\n",
    "                if next_class[currtag[-1][1]]==0:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            if output[-1] in output[:-1]:\n",
    "                output.pop(-1)\n",
    "            else:\n",
    "                for words in output[:-1]:\n",
    "                    [minus,plus]=diff(output[-1],words)\n",
    "                    if minus<=1 and minus+plus<=3:\n",
    "                        output.pop(-1)\n",
    "    if len(word)==1 or changeword==1:\n",
    "        word_list=[w for w in output]\n",
    "        if output==[]:\n",
    "            word_list=[w for w in word]\n",
    "    else:\n",
    "        word_list=[' '.join(word[:-1])+ \" \"+ w for w in output]\n",
    "    if word_list==[]:\n",
    "        return []\n",
    "    if len(word_list)>1:\n",
    "        new_prescription=[]\n",
    "        for i in word_list.copy():\n",
    "            new_pres=prescription_prediction(i,newwords)\n",
    "            if new_pres==[]:\n",
    "                ipos=word_list.index(i)\n",
    "                word_list.pop(ipos)\n",
    "                continue\n",
    "            new_prescription.append(new_pres)\n",
    "        if (sum(x) for x in new_prescription)==0:\n",
    "            return []\n",
    "        \n",
    "        #save columnwise:\n",
    "        if len(new_prescription)==1:\n",
    "            final_new_prescription=[]\n",
    "            for ij in range(min(newwords,len(new_prescription[0]))):\n",
    "                if new_prescription[0][ij] not in currentsuggestions:\n",
    "                    final_new_prescription.append(new_prescription[0][ij])\n",
    "        elif len(new_prescription)==0:\n",
    "            return []\n",
    "        else:\n",
    "            prescribeorder=[]\n",
    "            maxlen=max(len(x) for x in new_prescription)\n",
    "            i=0\n",
    "            for i in range(maxlen):\n",
    "                for j in range(len(new_prescription)):\n",
    "                    try:\n",
    "                        prescribeorder.append(new_prescription[j][i])\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            final_new_prescription=[]\n",
    "            i=0\n",
    "            while len(final_new_prescription)<newwords and i<30:\n",
    "                try:\n",
    "                    if isvalidprescription(tagg.tag(prescribeorder[i].split()))==1:\n",
    "                        i+=1\n",
    "                        continue\n",
    "                    if prescribeorder[i] not in currentsuggestions:\n",
    "                        final_new_prescription.append(prescribeorder[i])\n",
    "                except:\n",
    "                    if i==len(prescribeorder)-1:\n",
    "                        break\n",
    "                i+=1\n",
    "    else:\n",
    "        new_prescription=prescription_prediction(word_list[0],newwords)\n",
    "        final_new_prescription=[]\n",
    "        i=0\n",
    "        while len(final_new_prescription)<newwords and i<30:\n",
    "            if i==len(new_prescription):\n",
    "                break\n",
    "            if isvalidprescription(tagg.tag(new_prescription[i].split()))==1:\n",
    "                i+=1\n",
    "                continue\n",
    "            if new_prescription[i] not in currentsuggestions:\n",
    "                final_new_prescription.append(new_prescription[i])\n",
    "            i+=1\n",
    "    \n",
    "    inp_split_test=inpstr.split()\n",
    "    changed=0\n",
    "    for val,j in enumerate(inp_split_test[:-1]):\n",
    "        try:\n",
    "            syn=synonyms[j]\n",
    "            inp_split_test[val]=syn\n",
    "            if syn!=j:\n",
    "                changed=1\n",
    "        except:\n",
    "            changed+=0\n",
    "    if changed==1:\n",
    "        new_pres=' '.join(inp_split_test)\n",
    "        if new_pres not in final_new_prescription and new_pres not in currentsuggestions:\n",
    "            final_new_prescription.insert(0,new_pres)\n",
    "            if len(final_new_prescription)>newwords:\n",
    "                final_new_prescription=final_new_prescription[:newwords]\n",
    "\n",
    "    return final_new_prescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_color=\"#F1948A\"\n",
    "moderate_color=\"#F7DC6f\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEntry(Entry):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        Entry.__init__(self, *args, **kwargs)\n",
    "        self.var = self[\"textvariable\"]\n",
    "        if self.var == '':\n",
    "            self.var = self[\"textvariable\"] = StringVar()\n",
    "\n",
    "        self.var.trace('w', self.changed)\n",
    "        self.bind(\"<Right>\", self.selection)\n",
    "        self.bind(\"<Up>\", self.up)\n",
    "        self.bind(\"<Down>\", self.down)\n",
    "        self.bind(\"<Button-1>\",self.close)\n",
    "        self.lb_up = False\n",
    "        self.tagg=tagg\n",
    "        self.lastlen=0\n",
    "        self.severe_int=[]\n",
    "        self.moderate_int=[]\n",
    "        self.interaction=0 #0.. don't show interactions\n",
    "        self.severity=[0,0,0,0,0,0]\n",
    "        self.sev_list=[]\n",
    "        self.mod_list=[]\n",
    "\n",
    "    def changed(self, name, index, mode):  \n",
    "        if len(self.var.get()) <=2:\n",
    "            if self.lb_up==True:\n",
    "                self.lb.destroy()\n",
    "                self.lb_up=False\n",
    "            return\n",
    "        else:\n",
    "            change=0\n",
    "            selfvar=self.var.get()\n",
    "            if self.lastlen>len(selfvar):\n",
    "                if selfvar[-1]==' ':\n",
    "                    change=1\n",
    "                elif len(re.findall(' ',selfvar))==0:\n",
    "                    change=0\n",
    "            self.lastlen=len(self.var.get())\n",
    "            words=[]\n",
    "            \n",
    "            newwords=6\n",
    "            if self.lb_up==True and change==0:\n",
    "                varlen=len(selfvar)\n",
    "                for val,jkl in enumerate(self.lb.get(0,END)):\n",
    "                    if jkl[:varlen]==selfvar:\n",
    "                        words.append(jkl)\n",
    "                        self.severity[len(words)-1]=self.severity[val]\n",
    "                if len(words)>=6:\n",
    "                    return\n",
    "                newwords=max(0,6-len(words))\n",
    "            oldwordlen=len(words)\n",
    "            new_word_suggestions=word_prediction(selfvar,self.tagg,words,newwords)\n",
    "            words.extend(new_word_suggestions)\n",
    "            newwordlen=len(words)\n",
    "            if self.interaction==1:\n",
    "                for i in range(oldwordlen,newwordlen):\n",
    "                    currtag=tagg.tag(words[i].split())\n",
    "\n",
    "                    if 'DN' in [v for k,v in currtag]:\n",
    "                        currdrugname=[k for k,v in currtag if v=='DN']\n",
    "                        if ' '.join(currdrugname) in self.severe_int:\n",
    "                            self.severity[i]=2\n",
    "                        elif ' '.join(currdrugname) in self.moderate_int:\n",
    "                            self.severity[i]=1\n",
    "                        else:\n",
    "                            self.severity[i]=0\n",
    "                            \n",
    "            #new_word at pos 1 if only synonyms have been changed:\n",
    "            if len(new_word_suggestions)>0:\n",
    "                new_first_len=len(new_word_suggestions[0].split())\n",
    "                input_len=len(selfvar.split())\n",
    "                if input_len==new_first_len:\n",
    "                    syn_word=words[0]\n",
    "                    syn_sev=self.severity[0]\n",
    "                    words[0]=words[oldwordlen]\n",
    "                    self.severity[0]=self.severity[oldwordlen]\n",
    "                    words[oldwordlen]=syn_word\n",
    "                    self.severity[oldwordlen]=syn_sev\n",
    "                        \n",
    "            if words:            \n",
    "                if not self.lb_up:\n",
    "                    self.lb = Listbox(height=6,width=74)\n",
    "                    self.lb.bind(\"<Double-Button-1>\", self.selection)\n",
    "                    self.lb.bind(\"<Right>\", self.selection)\n",
    "                    self.lb.place(x=self.winfo_x(), y=self.winfo_y()+self.winfo_height())\n",
    "                    self.lb_up = True\n",
    "                \n",
    "                self.lb.delete(0, END)\n",
    "                for val,w in enumerate(words):\n",
    "                    self.lb.insert(END,w)\n",
    "                    if self.interaction==1:\n",
    "                        severi=self.severity[val]\n",
    "                        if severi==0:\n",
    "                            self.lb.itemconfig(val,{'bg':'White'})\n",
    "                            self.lb.itemconfig(val,{'fg':'Black'})\n",
    "                        elif severi==1:\n",
    "                            self.lb.itemconfig(val,{'bg':moderate_color})\n",
    "                            self.lb.itemconfig(val,{'fg':'White'})\n",
    "                        else:\n",
    "                            self.lb.itemconfig(val,{'bg':severe_color})\n",
    "                            self.lb.itemconfig(val,{'fg':'White'})\n",
    "                self.lb.config(height=newwordlen)\n",
    "            else:\n",
    "                if self.lb_up:\n",
    "                    self.lb.destroy()\n",
    "                    self.lb_up = False\n",
    "            self.lastlen=len(self.var.get())\n",
    "        \n",
    "    def selection(self, event):\n",
    "\n",
    "        if self.lb_up:\n",
    "            self.var.set(self.lb.get(ACTIVE))\n",
    "            self.lb.destroy()\n",
    "            self.lb_up = False\n",
    "            self.icursor(END)\n",
    "    \n",
    "    def close(self,event):\n",
    "        if self.lb_up:\n",
    "            self.lb.destroy()\n",
    "            self.lb_up=False\n",
    "            self.icursor(END)\n",
    "\n",
    "    def up(self, event):\n",
    "\n",
    "        if self.lb_up:\n",
    "            if self.lb.curselection() == ():\n",
    "                index = '0'\n",
    "            else:\n",
    "                index = self.lb.curselection()[0]\n",
    "            if index != '0':                \n",
    "                self.lb.selection_clear(first=index)\n",
    "                index = str(int(index)-1)                \n",
    "                self.lb.selection_set(first=index)\n",
    "                self.lb.activate(index) \n",
    "    \n",
    "    def down(self, event):\n",
    "\n",
    "        if self.lb_up:\n",
    "            if self.lb.curselection() == ():\n",
    "                index = '0'\n",
    "            else:\n",
    "                index = self.lb.curselection()[0]\n",
    "            if index != END:                        \n",
    "                self.lb.selection_clear(first=index)\n",
    "                index = str(int(index)+1)        \n",
    "                self.lb.selection_set(first=index)\n",
    "                self.lb.activate(index) \n",
    "\n",
    "    def splitup(self):#,event):\n",
    "        dns_text.set('')\n",
    "        dns.config(bg=\"SystemButtonFace\")\n",
    "        dns.config(fg=\"SystemButtonFace\")\n",
    "        if self.lb_up:\n",
    "            self.lb.destroy()\n",
    "            self.lb_up = False\n",
    "        Warning.set(\"\")\n",
    "        for k,v in new_dict.items():\n",
    "            v.set(\"\")\n",
    "        inptext=self.var.get()\n",
    "        if inptext=='':\n",
    "            return\n",
    "        word=inptext.lower().split()\n",
    "        currtag=tagg.tag(word)\n",
    "        if isvalidprescription(currtag)==1:\n",
    "            Warning.set('Warning: Invalid')\n",
    "        else:\n",
    "            classes=[currtag[0][1]]\n",
    "            newtag=[currtag[0][0]]\n",
    "            for i in range(1,len(currtag)):\n",
    "                if currtag[i][1]==currtag[i-1][1]:\n",
    "                    newtag[-1]=' '.join((newtag[-1],currtag[i][0]))\n",
    "                else:\n",
    "                    newtag.append(currtag[i][0])\n",
    "                    classes.append(currtag[i][1])\n",
    "            for i,val in enumerate(classes):\n",
    "                strsyn=newtag[i]\n",
    "                if str(val) in [\"ST\",\"MA\",\"MI\"]:\n",
    "                    new_dict[val].set(\" \".join(strsyn.split()[1:]))\n",
    "                    continue\n",
    "                new_dict[val].set(strsyn)\n",
    "                if val==\"DN\" and self.interaction==1:\n",
    "                    if word[i] in self.severe_int:\n",
    "                        dns_text.set(self.sev_list[word[i]])\n",
    "                        dns.config(bg=severe_color)\n",
    "                        dns.config(fg=\"white\")\n",
    "                    elif word[i] in self.moderate_int:\n",
    "                        dns_text.set(self.mod_list[word[i]])\n",
    "                        dns.config(bg=moderate_color)\n",
    "                        dns.config(fg=\"white\")\n",
    "        return\n",
    "        \n",
    "    def clear(self):\n",
    "        self.var.set('')\n",
    "        for k,v in new_dict.items():\n",
    "            v.set(\"\")\n",
    "        Warning.set(\"\")\n",
    "        Patient.set(\"\")\n",
    "        Patient1.set(\"\")\n",
    "        self.severe_int=[]\n",
    "        self.moderate_int=[]\n",
    "        sev.config(bg=\"SystemButtonFace\") \n",
    "        mod.config(bg=\"SystemButtonFace\")\n",
    "        sev.config(fg=\"SystemButtonFace\")\n",
    "        mod.config(fg=\"SystemButtonFace\")\n",
    "        var.set(0)\n",
    "        dns_text.set('')\n",
    "        dns.config(bg=\"SystemButtonFace\")\n",
    "        dns.config(fg=\"SystemButtonFace\")\n",
    "    \n",
    "    def newpat(self):\n",
    "        n_drugs=randrange(10)\n",
    "        curr_patient=[]\n",
    "        severe_interactions=[]\n",
    "        moderate_interactions=[]\n",
    "        severe_int_expl={}\n",
    "        moderate_int_expl={}\n",
    "        while len(curr_patient)<n_drugs:\n",
    "            drug=randrange(len(intlist))\n",
    "            if intlist[drug] not in curr_patient:\n",
    "                curr_patient.append(intlist[drug])\n",
    "                for keys in interactions.keys():\n",
    "                    if intlist[drug] in interactions[keys].keys():\n",
    "                        severity=interactions[keys][intlist[drug]][0]\n",
    "                        if severity=='Severe':\n",
    "                            if keys not in severe_interactions:\n",
    "                                severe_interactions.append(keys)\n",
    "                                severe_int_expl[keys]=interactions[keys][intlist[drug]][1]\n",
    "                        elif severity=='Moderate':\n",
    "                            if keys not in moderate_interactions:\n",
    "                                moderate_interactions.append(keys)\n",
    "                                moderate_int_expl[keys]=interactions[keys][intlist[drug]][1]\n",
    "        self.severe_int=severe_interactions\n",
    "        self.moderate_int=moderate_interactions\n",
    "        self.sev_list=severe_int_expl\n",
    "        self.mod_list=moderate_int_expl\n",
    "        Patient1.set(' '.join(curr_patient))\n",
    "        if var.get()==0:\n",
    "            var.set(1)\n",
    "            self.yes()\n",
    "            \n",
    "        #print(self.severe_int)\n",
    "        #print(self.moderate_int)\n",
    "        \n",
    "    def yes(self):\n",
    "        self.interaction=var.get()\n",
    "        if self.interaction==1:\n",
    "            self.newpat()\n",
    "            sev.config(bg=severe_color) \n",
    "            mod.config(bg=moderate_color)\n",
    "            sev.config(fg=\"white\")\n",
    "            mod.config(fg=\"white\")\n",
    "            Patient.set('Patients Current Drugs:')\n",
    "            Patient1.set(' '.join(curr_patient))\n",
    "            \n",
    "            \n",
    "            moderate.set('Severity: Moderate')\n",
    "            severe.set('Severity: Severe')\n",
    "        else:\n",
    "            sev.config(bg=\"SystemButtonFace\") \n",
    "            mod.config(bg=\"SystemButtonFace\")\n",
    "            sev.config(fg=\"SystemButtonFace\")\n",
    "            mod.config(fg=\"SystemButtonFace\")\n",
    "            Patient.set('')\n",
    "            Patient1.set('')\n",
    "            moderate.set('')\n",
    "            severe.set('')\n",
    "            self.severity=[0,0,0,0,0,0]\n",
    "            dns_text.set('')\n",
    "            dns.config(bg=\"SystemButtonFace\")\n",
    "            dns.config(fg=\"SystemButtonFace\")\n",
    "\n",
    "fields = ['Drug Name', 'Dose', 'Unit', 'Starting','Frequency','Route','Maximum','Minimum','Duration']\n",
    "\n",
    "root=Tk()\n",
    "presc=StringVar()\n",
    "DN=StringVar()\n",
    "DO=StringVar()\n",
    "UN=StringVar()\n",
    "ST=StringVar()\n",
    "FR=StringVar()\n",
    "RO=StringVar()\n",
    "MA=StringVar()\n",
    "MI=StringVar()\n",
    "DU=StringVar()\n",
    "Warning=StringVar()\n",
    "\n",
    "new_dict={'DN':DN,'DO':DO,'UN':UN,'ST':ST,'FR':FR,'RO':RO,'MA':MA,'MI':MI,'DU':DU}\n",
    "new_dict_names={'DN':'Drug Name: ','DO':'Dose: ','UN':'Unit: ','ST':'Starting: ','FR':'Frequency: ','RO':'Route: ','MA':'Maximum: ','MI':'Minimum: ','DU':'Duration: '}\n",
    "root.geometry('600x650')\n",
    "root.title('Electronic Prescription Project')\n",
    "\n",
    "row = Frame(root)\n",
    "lab = Label(row, width=15, text='Prescription:', anchor='w')\n",
    "ent = AutoEntry(row)#,textvariable=presc)\n",
    "row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "lab.pack(side=LEFT)\n",
    "ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "\n",
    "\n",
    "row=Frame(root)\n",
    "mbutton=Button(row,text='Split Up',command=ent.splitup)\n",
    "mbutton2=Button(row,text='Clear all',command=ent.clear)\n",
    "var = IntVar()\n",
    "checkbox = Checkbutton(row,text='Interactions',variable=var, command=ent.yes)\n",
    "mbutton3=Button(row,text='New Patient',command=ent.newpat)\n",
    "random.seed(5)\n",
    "row.pack(side=TOP)\n",
    "mbutton.pack(side=LEFT,padx=10,pady=10)\n",
    "checkbox.pack(side=RIGHT,padx=10,pady=10)\n",
    "mbutton3.pack(side=RIGHT,padx=10,pady=10)\n",
    "mbutton2.pack(side=LEFT,padx=10,pady=10)\n",
    "\n",
    "\n",
    "Label(root,textvariable=Warning,font=(\"Courier\",20)).pack()\n",
    "\n",
    "for k,v in new_dict.items():\n",
    "    row = Frame(root)\n",
    "    lab = Label(row, width=15, text=new_dict_names[k], anchor='w')\n",
    "    ent = Entry(row,textvariable=v)\n",
    "    row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "    lab.pack(side=LEFT)\n",
    "    ent.pack(side=LEFT, expand=NO, fill=X)\n",
    "    if k=='DN':\n",
    "        dns_text=StringVar()\n",
    "        dns=Label(row,wraplength=300,width=300,textvariable=dns_text)\n",
    "        dns.pack(side=RIGHT)\n",
    "\n",
    "    \n",
    "Patient=StringVar()\n",
    "pat=Label(root,textvariable=Patient,font=(\"Courier\",10))\n",
    "pat.pack(side=TOP,padx=10,pady=10)\n",
    "Patient1=StringVar()\n",
    "pat1=Label(root,wraplength=400,textvariable=Patient1,font=(\"Courier\",10))\n",
    "pat1.pack(side=TOP)\n",
    "severe=StringVar()\n",
    "sev=Label(root,textvariable=severe,font=(\"Courier\",10))\n",
    "sev.pack(side=LEFT,padx=20)\n",
    "moderate=StringVar()\n",
    "mod=Label(root,textvariable=moderate,font=(\"Courier\",10))\n",
    "mod.pack(side=LEFT,pady=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "root.mainloop()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
